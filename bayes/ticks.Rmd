---
title: Stan Example
subtitle: QERM 597 Wi 21
date: January 27, 2021
---

# Preparation

## Packages

We'll use the `rstan` package (an alternative would be the `rcmdstan` package),
and the `tidyverse`. Note that there are 

```{r}
library(rstan)
library(tidyverse)

## Use parallel cores for different chains as recommended in the rstan package
## message
options(mc.cores = parallel::detectCores())
```

```{r echo=FALSE}
## Options for knitting
options(digits = 2)
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      cache = TRUE,
                      fig.height = 6,
                      fig.width = 8)
```


## Dataset

Here we'll use the `grouseticks` data set, which is built into the `lme4` data set.

```{r}
data(grouseticks, package = "lme4")
```

The response we're interested in is `TICKS`, the number of ticks on the head of
a grouse chick. Other covariates include:

- `BROOD`: brood code
- `HEIGHT`: height of the chick
- `YEAR`: two-digit year
- `LOCATION`: location code for the nest
- `cHEIGHT`: centered height (mean subtracted off)

A quick exploratory analysis reveals that the number of ticks varies over almost
two orders of magnitude, with many zeros. The overall mean number of ticks is
`mean(grouseticks$TICKS)`, while the variance is `var(grouseticks$TICKS)`. This
indicates that the stochastic process that generated these observations was not
simply Poisson; the variance is much too large.

```{r}
ggplot(grouseticks, aes(x = TICKS)) +
  geom_histogram(stat = "count")
```

Note that broods with more ticks also have larger variance in number of ticks.
So a Poisson process with some covariates may be appropriate.

```{r}
ggplot(grouseticks, aes(x = BROOD, y = TICKS, color = LOCATION)) +
  geom_point()
```

Number of ticks appears to be inversely related to chick height. 

```{r}
ggplot(grouseticks, aes(x = HEIGHT, y = log(TICKS))) +
  geom_point() +
  geom_smooth(method = lm)
```

# The model

I tend to write my models "in reverse", from the observations back to the
parameter values. New values would be simulated "forward" from parameter values
to observations.

We will model this as a Poisson GLM with a log link. The number of ticks in the
$i\text{th}$ observation, $Y_i$, is then

$$Y_{i} \sim \operatorname{Poisson}(\lambda_{i})$$

Where the value of $\lambda_{i}$ depends on the intercept $\mu$, (centered)
chick height $h_{i}$ and the parameter $\delta$, the effect of location $l$,
$\gamma_{l}$, and the effect of brood $b$, $\beta_{b}$. One way to consider the
$\gamma$ and $\beta$ parameters is that we expect individuals to be more similar
if they are in the same location, and even more similar if they are in the same
brood. Then we have

$$\lambda_{i} = \exp(\mu + \delta h_{i} + \gamma_{i} + \beta_{i}).$$

Note that this is an equality statement rather than a "sampling statement" as
above. Now we need to place priors on the parameters $\mu$, and $\delta$, as
well as each of the $\gamma$ and $\beta$ parameters. Not including a prior puts
an implicit "flat" prior on the parameter in Stan, and may cause computational
issues. It is also not always as uninformative as it appears.

Probably the best way to get a feel for priors is to use "prior predictive
checks". Here we will use a prior predictive check for $\mu$. It is possible to
integrate this into your Stan model to avoid duplicating code, but it's easy
enough here to explore reasonable values for $\mu$ using the random number
generators in R. We will generate a value for $\mu$ from our prior, then use
that to generate multiple datasets to see if they look "reasonable". We'll place
a normal prior on $\mu$, so we need to decide on a mean and a standard deviation
to parameterize the prior.

```{r}
mu_priorpred <- function(n, mean, sd) {
  mu <- rnorm(1, mean, sd)
  rpois(n, exp(mu))
}

mu_pp_hist <- function(m, mean, sd) {
  df <- tibble(ticks = c(replicate(m, mu_priorpred(403, mean, sd))))
  ggplot(df, aes(x = ticks)) +
    geom_histogram(stat = "count")
}
```

Let's start with a $\operatorname{normal}(0, 10)$ prior; that seems pretty
uninformative right?

```{r}
mu_pp_hist(100, 0, 10)
```

Uh oh. You can't see any of the histogram bars because the x-axis ranges over 9
order of magnitude. We're putting some prior mass (you can think of it as some
belief) that the number of ticks on a chick's head is in the billions range.
Exponentials are dangerous! We could reduce the mean, but the standard deviation
is going to be more important here.

```{r}
mu_pp_hist(1000, 0, 1)
```

Hmm... this looks a bit too "tight" to me. It does make sense that there might
be many chicks with zero ticks on their head though. Maybe slightly increase the
mean and the variance of the prior.

```{r}
mu_pp_hist(1000, 1, 1.25)
```

This prior places most of the mass at low numbers, but has a long tail. Note
that we expect more variance in our observations once we include our covariates. So

$$\mu \sim \operatorname{normal}(1, 1.25^{2})$$

A normal prior centered at zero seems reasonable for our $\delta$ parameter.

$$\delta \sim \operatorname{normal}(0, 1)$$

We can place similar priors on the $\gamma$ and $\beta$ parameters (though it would be better to place a *joint* prior on all these prior simultaneously).

$$\gamma \sim \operatorname{MVN}(0, \sigma_{\gamma}^{2}\boldsymbol{I})$$

Of course, now we need a prior for $\sigma_\gamma$. Following the penalized
complexity (PC) prior framework, I am choosing an exponential prior so that
$\Pr(\sigma_\gamma > 0.3) = 0.05)$. This has the effect of "shrinking" the
posterior toward a base model where location has no effect. **NOTE** that it is
important to double check the parameterization of distributions. The
exponential, for example, can be parameterized in terms of its *rate* or its
*mean*. Stan uses the *rate*, as do the `dexp`, `pexp`, `qexp`, and `rexp`
functions in R.

$$\sigma_{\gamma} \sim \operatorname{exponential}(10)$$

## The full model

$$Y_{i} \sim \operatorname{Poisson}(\lambda_{i})$$
$$\lambda_{i} = \exp(\mu + \delta h_{i} + \gamma_{g})$$
$$\mu \sim \operatorname{normal}(1, 1.25^{2})$$
$$\delta \sim \operatorname{normal}(0, 1)$$
$$\gamma \sim \operatorname{MVN}(0, \sigma_{\gamma}^{2}\boldsymbol{I})$$
$$\sigma_\gamma \sim \operatorname{exponential}(10)$$

It is preferred to use a separate file for a Stan model, so fill in the blanks
(`____`) in `ticks.stan` to match this model. It may not all match up perfectly
because there are vectorized functions available.

## A prior predictive check in R

Now that we have a model fully specified, we can do a prior predictive check. In
this case I will just write a function in R. We can use the same data set that
we'll pass to Stan later. It is also possible to write prior predictive checks
in Stan.

We construct the data as a `list` with elements matching the names declared
in `ticks.stan`. I am proprocessing the data by:

-_Rescaling `cHEIGHT` so that it has standard deviation one as well as mean
zero, then making sure it is a vector rather than a matrix; this makes setting a
prior on $\delta$ easier.
- Note that we need to reindexing `BROOD` and `LOCATION` so that they use
consecutive numbers starting from one. I do this by converting it to a factor
and then converting the factor to an integer.

```{r}
ticks_data <- list(nobs = nrow(grouseticks),
                   nlocations = length(unique(grouseticks$LOCATION)),
                   nbroods = length(unique(grouseticks$BROOD)),
                   ticks = grouseticks$TICKS,
                   cheight = c(scale(grouseticks$cHEIGHT)),
                   location = as.integer(factor(grouseticks$LOCATION)),
                   brood = as.integer(factor(grouseticks$BROOD)))

ticks_priorpred <- function(ticks_data) {
  sig_gamma <- rexp(1, 10)
  gamma <- rnorm(ticks_data$nlocations, 0, sig_gamma)
  delta <- rnorm(1, 0, 0.5)
  mu <- rnorm(1, 1.25)
  lambda <- exp(mu + delta * ticks_data$cheight + gamma[ticks_data$location])
  rpois(ticks_data$nobs, lambda)
}
```

A histogram of the resulting tick counts looks reasonable, with most chicks
having zero or few ticks, but with a long tail to allow for chicks that may have
severe infestations.

```{r}
pp <- data.frame(ticks = as.vector(replicate(100, ticks_priorpred(ticks_data))))
ggplot(pp, aes(x = ticks)) +
  geom_histogram(stat = "count")
```

# Run it!

To avoid warnings about low effective sample sizes, I increase the number of
iterations to 4,000 and the warmup to 2,000.

```{r}
ticks_fit <- stan("ticks_model_complete.stan",
                  data = ticks_data,
                  chains = 4, iter = 4000, warmup = 2000)
```

First we can check the summary of the fit:

```{r}
ticks_fit
```

Uh oh! Warnings of low effective sample sizes! First we'll check the traceplots.

```{r}
traceplot(ticks_fit)
```

This could be an opportunity to identify why the sampler is having difficulty,
which might indicate a missing covariate, correlated parameters, or other
issues. The `ShinyStan` package is helpful for interactive evaluation like this.

We can also check our posterior predictive simulations to evaluate model fit. It
takes some manipulation of the posterior object that Stan produces.

```{r warning=FALSE}
ticks_postpred <- as.data.frame(ticks_fit, pars = "sim_ticks") %>%
  pivot_longer(cols = starts_with("sim_ticks"),
               names_to = "INDEX",
               names_pattern = "sim_ticks\\[(.*)\\]",
               values_to = "TICKS") %>%
  left_join(select(grouseticks, INDEX, LOCATION),
            by = "INDEX")
```

We can look at the results graphically, and compare them to the data.

```{r}
locs <- 1:21
ticks_postpred %>%
  filter(LOCATION %in% locs) %>%
ggplot(aes(x = TICKS)) +
  geom_histogram(stat = "count") +
  geom_vline(aes(xintercept = TICKS),
             data = filter(grouseticks, LOCATION %in% locs),
             position = position_jitter(width = 0.8), alpha = 0.5) +
  facet_wrap(~ LOCATION, scale = "free_x")
```

```{r}
locs <- 22:42
ticks_postpred %>%
  filter(LOCATION %in% locs) %>%
ggplot(aes(x = TICKS)) +
  geom_histogram(stat = "count") +
  geom_vline(aes(xintercept = TICKS),
             data = filter(grouseticks, LOCATION %in% locs),
             position = position_jitter(width = 0.8), alpha = 0.5) +
  facet_wrap(~ LOCATION, scale = "free_x")
```

```{r}
locs <- 43:63
ticks_postpred %>%
  filter(LOCATION %in% 43:63) %>%
ggplot(aes(x = TICKS)) +
  geom_histogram(stat = "count") +
  geom_vline(aes(xintercept = TICKS),
             data = filter(grouseticks, LOCATION %in% locs),
             position = position_jitter(width = 0.8), alpha = 0.5) +
  facet_wrap(~ LOCATION, scale = "free_x")
```

It looks like accounting for variation by location isn't enough. It would be
worth checking whether allowing for variation by brood allows for a better fit.
This would be an interesting result in itself, potentially indicating that
susceptibility to ticks is brood-dependent more than location-dependent.

# Computing environment

```{r}
sessionInfo()
```
